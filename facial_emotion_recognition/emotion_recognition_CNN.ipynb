{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 처리 라이브러리\n",
    "import os\n",
    "import os.path as pth\n",
    "#from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "# import cv2\n",
    "\n",
    "# import mediapipe as mp\n",
    "\n",
    "# Tensorflow 관련 라이브러리\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Dropout, BatchNormalization, Flatten, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# GPU 설정\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 특정 GPU에 1GB 메모리만 할당하도록 제한\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        \n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        \n",
    "#         tf.config.experimental.set_virtual_device_configuration(\n",
    "#             gpus[0],\n",
    "#             [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=15000)])\n",
    "    except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TFrecord 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "train_tfrecord_path = pth.join('./data/emotion_img_tf2', 'tf_record_train.tfrecords')\n",
    "valid_tfrecord_path = pth.join('./data/emotion_img_tf2', 'tf_record_train.tfrecords')\n",
    "\n",
    "# BUFFER_SIZE, BATCH_SIZE\n",
    "BUFFER_SIZE = 100\n",
    "BATCH_SIZE = 30\n",
    "NUM_CLASS = 8\n",
    "\n",
    "\n",
    "\n",
    "image_feature_description = {\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    'img_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    # 'id': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "def map_func(target_record):\n",
    "    img = target_record['image_raw']\n",
    "    label = int(target_record['img_id'])\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    return img, label\n",
    "\n",
    "def prep_func(image, label):\n",
    "    print(type(label))\n",
    "\n",
    "    result_image = image / 255\n",
    "    result_image = tf.image.resize(result_image, (270,480))\n",
    "\n",
    "    onehot_label = tf.one_hot(label, depth=NUM_CLASS)\n",
    "    return result_image, onehot_label\n",
    "\n",
    "    \n",
    "\n",
    "dataset = tf.data.TFRecordDataset(train_tfrecord_path, compression_type='GZIP')\n",
    "dataset = dataset.map(_parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.map(prep_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_dataset = tf.data.TFRecordDataset(valid_tfrecord_path, compression_type='GZIP')\n",
    "valid_dataset = valid_dataset.map(_parse_image_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.shuffle(BUFFER_SIZE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE)\n",
    "valid_dataset = valid_dataset.map(prep_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(patience=5)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor ='val_accuracy',\n",
    "                                            patience = 2,\n",
    "                                            factor = 0.5,\n",
    "                                            min_lr = 1e-7,\n",
    "                                            verbose = 1)\n",
    "\n",
    "model_check = ModelCheckpoint(filepath = './model/facial_emotion_cnn_270_480_adam_h5',\n",
    "                              monitor = 'val_loss',\n",
    "                              save_best_only = True)\n",
    "\n",
    "callbacks = [earlystop, learning_rate_reduction, model_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Base CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 268, 478, 8)       224       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 268, 478, 8)       32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 134, 239, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 134, 239, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 132, 237, 16)      1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 132, 237, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 66, 118, 16)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 66, 118, 16)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 124608)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                3987488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 3,989,368\n",
      "Trainable params: 3,989,256\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(8, (3, 3), activation='relu', input_shape=(270, 480, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "   2516/Unknown - 954s 379ms/step - loss: 0.9215 - accuracy: 0.5530INFO:tensorflow:Assets written to: ./model/facial_emotion_cnn_270_480_adam_h5/assets\n",
      "2516/2516 [==============================] - 1898s 754ms/step - loss: 0.9215 - accuracy: 0.5530 - val_loss: 3.8878 - val_accuracy: 0.1607 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "2516/2516 [==============================] - 1894s 753ms/step - loss: 1.0625 - accuracy: 0.4585 - val_loss: 2386.1421 - val_accuracy: 0.1299 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "2516/2516 [==============================] - ETA: 0s - loss: 1.5540 - accuracy: 0.2355"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(dataset,\n",
    "                        epochs=1000,\n",
    "                        validation_data=valid_dataset,\n",
    "                        callbacks = callbacks\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env_TF2] *",
   "language": "python",
   "name": "conda-env-data_env_TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
